{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "%autoreload 2\n",
    "\n",
    "from nesta.packages.geo_utils.country_iso_code import country_iso_code_to_name\n",
    "from nih_topics import get_nih_projects\n",
    "from cordis_topics import get_cordis_projects\n",
    "#from crunchbase_topics import get_crunchbase_orgs\n",
    "from arxiv_topics import get_arxiv_articles\n",
    "\n",
    "from indicators.core.nuts_utils import get_nuts_info_lookup\n",
    "from indicators.core.nlp_utils import parse_corex_topics\n",
    "import pandas as pd\n",
    "from datetime import datetime as dt\n",
    "from functools import partial\n",
    "from skbio.diversity.alpha import shannon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_objects_and_labels(data_getter, topic_model_path):\n",
    "    _objects = list(data_getter())[0]\n",
    "    objects = pd.DataFrame(_objects)\n",
    "    topics = parse_corex_topics(topic_model_path.format(\"topics.txt\"))\n",
    "    labels = pd.read_csv(topic_model_path.format(\"labels.txt\"), names=topics, index_col=0)\n",
    "    total_correlation = pd.read_csv(topic_model_path.format(\"most_deterministic_groups.txt\"))\n",
    "    is_fluffy = total_correlation[\" NTC\"].apply(lambda x: abs(x) < 0.02)\n",
    "    fluffy_topics = [topics[itopic] for itopic in total_correlation['Group num.'].loc[is_fluffy]]                     \n",
    "    non_stop_topics = pd.Series(topics,index=topics)[(labels.mean(axis=0) < 0.3)].values.tolist()\n",
    "    antitopics = [t for t in topics if t.count(\"~\") >= 3]\n",
    "    labels = labels[(set(non_stop_topics) - set(antitopics)) - set(fluffy_topics)]\n",
    "    return objects, labels\n",
    "\n",
    "def total_activity_by_topic_in_date_range(objs, labels, datefield, norm_days, from_date='2020-03-01', to_date=dt.now()):\n",
    "    _date = objs[datefield]\n",
    "    total_days = (pd.to_datetime(to_date) - pd.to_datetime(from_date)).days\n",
    "    norm = norm_days / total_days\n",
    "    slicer = (_date > pd.to_datetime(from_date)) & (_date < pd.to_datetime(to_date))\n",
    "    activity = labels[slicer].sum(axis=0).sort_values()\n",
    "    return norm * activity\n",
    "\n",
    "\n",
    "def thematic_diversity(objs, labels, datefield, covid_slicer, from_date='2020-03-01'):\n",
    "    _date = objs[datefield]\n",
    "    date_slicer = _date > pd.to_datetime(from_date)\n",
    "    thematic_diversity_covid = shannon(labels.loc[date_slicer & covid_slicer].sum(axis=0))\n",
    "    thematic_diversity_noncovid = shannon(labels.loc[date_slicer & ~covid_slicer].sum(axis=0))\n",
    "    return thematic_diversity_covid, thematic_diversity_noncovid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose_indicator_names = {\n",
    "                            \"total_activity\": 'Total activity ({}) since March 2020',\n",
    "                            \"relative_activity\": 'Activity ({}) since March 2020, relative to the expectation from 2015-2019',\n",
    "                            \"relative_activity_covid\": 'Covid-related activity ({}) since March 2020, relative to the expectation from 2015-2019',\n",
    "                            \"relative_activity_noncovid\": 'Non-covid-related activity ({}) since March 2020, relative to the expectation from 2015-2019',\n",
    "                            \"overrepresentation_activity\": 'Over-representation of covid-related activity ({}) since March 2020, compared to non-covid-related projects',\n",
    "                            \"thematic_diversity\": \"Shannon diversity of {}\"\n",
    "                          }\n",
    "                           \n",
    "                           \n",
    "\n",
    "def covid_filterer(item):\n",
    "    return any(term in item for term in\n",
    "               ['covid', 'covid-19', 'coronavirus'])\n",
    "\n",
    "def _generate_indicators(objects, topic_labels, datefield, old_from_date, old_to_date, weight_field, norm_days):\n",
    "    # Apply filter to data based on topic labels and date\n",
    "    covid_label = list(filter(covid_filterer, topic_labels))[0]\n",
    "    slicer = topic_labels[covid_label] == 1\n",
    "    weight = 1 if weight_field is None else objects[weight_field]\n",
    "    topic_labels = topic_labels.multiply(weight, axis=0)\n",
    "    get_total_activity_all = partial(total_activity_by_topic_in_date_range, objects, topic_labels, datefield, norm_days)\n",
    "    get_total_activity_cov = partial(total_activity_by_topic_in_date_range, objects.loc[slicer], topic_labels.loc[slicer], datefield, norm_days)\n",
    "    get_total_activity_noncov = partial(total_activity_by_topic_in_date_range, objects.loc[~slicer], topic_labels.loc[~slicer], datefield, norm_days)\n",
    "    old_dates = dict(from_date=old_from_date, to_date=old_to_date)\n",
    "\n",
    "    indicators = {}\n",
    "\n",
    "    # Levels of activity by topic, total (2020)\n",
    "    indicators[\"total_activity\"] = get_total_activity_all()\n",
    "\n",
    "    # Levels of activity by topic, relative to the average from 2015-2019\n",
    "    _norm_past_activity = get_total_activity_all(**old_dates)\n",
    "    _norm_past_activity.loc[_norm_past_activity == 0] = 1 # In the case where there is no past activity, take 1 as an upper bound\n",
    "    indicators[\"relative_activity\"] = indicators[\"total_activity\"] / _norm_past_activity\n",
    "\n",
    "    # Levels of activity by topic, relative to the average from 2015-2019, covid tagged\n",
    "    _total_activity = get_total_activity_cov()\n",
    "    _norm_past_activity = get_total_activity_cov(**old_dates)\n",
    "    _norm_past_activity.loc[_norm_past_activity == 0] = 1 # In the case where there is no past activity, take 1 as an upper bound\n",
    "    indicators[\"relative_activity_covid\"] = _total_activity / _norm_past_activity\n",
    "\n",
    "    # Levels of activity by topic, relative to the average from 2015-2019, non-covid tagged\n",
    "    _total_activity = get_total_activity_noncov()    \n",
    "    _norm_past_activity = get_total_activity_noncov(**old_dates)\n",
    "    _norm_past_activity.loc[_norm_past_activity == 0] = 1 # In the case where there is no past activity, take 1 as an upper bound\n",
    "    indicators[\"relative_activity_noncovid\"] = _total_activity / _norm_past_activity\n",
    "\n",
    "    # Overrepresentation (activity) of covid tagged compared to non-covid tagged projects\n",
    "    denominator = indicators[\"relative_activity_noncovid\"].copy()\n",
    "    denominator.loc[denominator == 0] = 1 # In the case where there is no past activity, take 1 as an upper bound\n",
    "    indicators[\"overrepresentation_activity\"] = indicators[\"relative_activity_covid\"] / denominator\n",
    "\n",
    "    # Thematic diversity (activity) of co-occurring topics of covid tagged compared to non-covid tagged projects\n",
    "    thematic_diversity_covid, thematic_diversity_noncovid = thematic_diversity(objects, topic_labels, \n",
    "                                                                               datefield, slicer)\n",
    "    indicators[\"thematic_diversity\"] = {\"covid-related-projects\": thematic_diversity_covid,\n",
    "                                        \"non-covid-related-projects\": thematic_diversity_noncovid}\n",
    "    return indicators\n",
    "\n",
    "    \n",
    "def generate_indicators(data_getter, topic_model_path, datefield, old_from_date, old_to_date, covid_start, weight_field=None, geo_split=False):\n",
    "    covid_end = dt.now()\n",
    "    norm_days = (pd.to_datetime(covid_end) - pd.to_datetime(covid_start)).days\n",
    "    objects, topic_labels = get_objects_and_labels(data_getter, topic_model_path)\n",
    "    if geo_split:\n",
    "        indicators = {geo_label: _generate_indicators(objects.loc[geo_index], topic_labels.loc[geo_index], \n",
    "                                                      datefield, old_from_date, old_to_date, weight_field, norm_days)\n",
    "                      for geo_index, geo_label in data_getter(geo_split=True)}\n",
    "        return indicators\n",
    "    else:\n",
    "        return _generate_indicators(objects, topic_labels, datefield, old_from_date, old_to_date, weight_field, norm_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "from matplotlib import cm\n",
    "\n",
    "def make_parallels(activity):\n",
    "    parallels = defaultdict(list)\n",
    "    indicator_names = []\n",
    "    for indicator_name, indicators in activity.items():\n",
    "        if indicator_name.startswith('thematic_diversity'):\n",
    "            continue\n",
    "        if indicator_name == 'relative_activity_noncovid':\n",
    "            continue            \n",
    "        indicator_names.append(indicator_name)        \n",
    "        for i, (topic, value) in enumerate(indicators.sort_values().items()):\n",
    "            score = i/len(indicators)\n",
    "            parallels[topic].append(score)\n",
    "    return indicator_names, parallels\n",
    "\n",
    "def make_parallel_plot(activity, dataset, entity_type, activity_type='activity'):\n",
    "    cmap = cm.get_cmap('cividis')\n",
    "    verbose_indicator_names = {'total_activity': f'Total {activity_type} of topic\\nsince March 2020',\n",
    "                               'relative_activity': f'{activity_type.title()} of topic since March 2020,\\nrelative to expectation of 2015-2019',\n",
    "                               'relative_activity_covid': f'{activity_type.title()} of topic since March 2020,\\nrelative to expectation of 2015-2019,\\nof projects also tagged by Covid topic',\n",
    "                               'overrepresentation_activity': f'Overrepresentation (in terms of {activity_type.title()})\\nof this topic in Covid-tagged projects'}\n",
    "\n",
    "    indicator_names, parallels = make_parallels(activity)\n",
    "    indicator_names = [verbose_indicator_names[name] for name in indicator_names]\n",
    "    \n",
    "    # Remove topics which are too prevalent (\"stop topics\") (first element, v[0] is 'total_activity')\n",
    "    parallels = {k: v for k, v in parallels.items() if v[0] < 0.95 and v[0] > 0.05}\n",
    "    \n",
    "    df = pd.DataFrame(parallels, index=indicator_names).T.reset_index()\n",
    "    df['Topic'] = df['index']\n",
    "    del df['index']\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12,15))\n",
    "    order = [coords[-1] for coords in parallels.values()]\n",
    "    pd.plotting.parallel_coordinates(df, 'Topic', ax=ax, color=map(cmap, order))\n",
    "    handles, labels = ax.get_legend_handles_labels()\n",
    "    labels, handles, _ = zip(*sorted(zip(labels, handles, order), key=lambda t: -t[2]))\n",
    "    legend = ax.legend(handles, labels, bbox_to_anchor=(1.2, 1.05), title='Topics')\n",
    "    legend.get_title().set_fontsize(20)\n",
    "    ax.set_xlabel('Indicator name', fontsize=20)\n",
    "    ax.set_ylabel('Ranking of indicator', fontsize=20)\n",
    "    ax.set_title(f\"Topic composition of {entity_type.title()}\\nDataset: {dataset.title()}\\nIndicator of: {activity_type.title()} levels\", fontsize=25)\n",
    "    return ax\n",
    "\n",
    "def make_diversity_plot(diversities):\n",
    "    fig, ax = plt.subplots(figsize=(10,12))\n",
    "\n",
    "    cmap = cm.get_cmap('Set2')\n",
    "    tick_locations = []\n",
    "    for i, (dataset, (ymin, ymax)) in enumerate(diversities.items()):\n",
    "        tick_locations.append(i+0.5)\n",
    "        ax.vlines(i+0.5, ymin, ymax, colors='k', linestyles='--')\n",
    "        if i == 0:\n",
    "            ax.scatter(i+0.5, ymin, s=100, color=cmap(1), label='Covid-tagged')\n",
    "            ax.scatter(i+0.5, ymax, s=100, color=cmap(2), label='Non-covid-tagged')\n",
    "        else:\n",
    "            ax.scatter(i+0.5, ymin, s=100, color=cmap(1))\n",
    "            ax.scatter(i+0.5, ymax, s=100, color=cmap(2))        \n",
    "    ax.set_xticks(tick_locations)\n",
    "    ax.set_xticklabels(diversities.keys(), fontsize=20)\n",
    "    ax.set_ylabel('Topic diversity of data', fontsize=20)\n",
    "    ax.set_xlabel('Dataset', fontsize=30)\n",
    "    ax.legend(fontsize=20)\n",
    "    return ax\n",
    "\n",
    "def flatten(nested_dict):\n",
    "    items = []\n",
    "    for k, v in nested_dict.items():\n",
    "        if type(v) is pd.Series:\n",
    "            v = dict(v)\n",
    "        if type(v) is dict:\n",
    "            for v in flatten(v):\n",
    "                items.append((k, *v))\n",
    "        else:\n",
    "            items.append((k, v))\n",
    "    return items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Activity indicators: NiH\n",
    "# BASE_PATH = \"/Users/jklinger/Nesta/Pivot/indicators/two/topic_iteration/topic-model-{}-100-25\"\n",
    "# BASE_KWARGS = dict(old_from_date='2015-01-01', old_to_date='2020-01-01', covid_start='2020-03-01')\n",
    "\n",
    "# data_params = [{'dataset': 'cordis', 'entity_type': 'projects', 'data_getter': get_cordis_projects, 'datefield': 'start_date', 'weight_field':'funding'},\n",
    "#                #{'dataset': 'crunchbase', 'entity_type': 'organizations', 'data_getter': get_crunchbase_orgs, 'datefield': 'founded', 'weight_field':'funding'},]\n",
    "#                {'dataset': 'nih', 'entity_type': 'projects', 'data_getter': get_nih_projects, 'datefield': 'start_date', 'weight_field':'funding'},\n",
    "#                {'dataset': 'arxiv', 'entity_type': 'articles', 'data_getter': get_arxiv_articles, 'datefield': 'created', 'weight_field': None}]\n",
    "\n",
    "\n",
    "# diversities = {}\n",
    "# for params in data_params:\n",
    "#     weight_field = params.pop('weight_field')\n",
    "#     entity_type = params.pop('entity_type')\n",
    "#     dataset = params.pop('dataset')\n",
    "#     kwargs = dict(topic_model_path=BASE_PATH.format(dataset) + \"/{}\",\n",
    "#                   **BASE_KWARGS, **params)\n",
    "#     print(\"Doing\", dataset)\n",
    "#     activity_indicators = generate_indicators(**kwargs)\n",
    "#     diversities[dataset] = (activity_indicators[\"thematic_diversity_covid\"], activity_indicators[\"thematic_diversity_noncovid\"])\n",
    "    \n",
    "#     print(\"Making parallel plot\")\n",
    "#     make_parallel_plot(activity_indicators, dataset=dataset, entity_type=entity_type)    \n",
    "#     if weight_field is not None:\n",
    "#         funding_indicators = generate_indicators(weight_field=weight_field, **kwargs)\n",
    "#         make_parallel_plot(funding_indicators, dataset=dataset, entity_type=entity_type, activity_type='funding')\n",
    "\n",
    "# _ = make_diversity_map(diversites)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing cordis\n",
      "Doing nih\n",
      "Doing arxiv\n"
     ]
    }
   ],
   "source": [
    "# Activity indicators: NiH\n",
    "BASE_PATH = \"/Users/jklinger/Nesta/Pivot/indicators/two/topic_iteration/topic-model-{}-100-25\"\n",
    "BASE_KWARGS = dict(old_from_date='2015-01-01', old_to_date='2020-01-01', covid_start='2020-03-01')\n",
    "\n",
    "data_params = [{'dataset': 'cordis', 'entity_type': 'projects', 'data_getter': get_cordis_projects, 'datefield': 'start_date', 'weight_field':'funding', 'geo_split': True},\n",
    "               {'dataset': 'nih', 'entity_type': 'projects', 'data_getter': get_nih_projects, 'datefield': 'start_date', 'weight_field':'funding', 'geo_split': True},\n",
    "               {'dataset': 'arxiv', 'entity_type': 'articles', 'data_getter': get_arxiv_articles, 'datefield': 'created', 'weight_field': None, 'geo_split': True}]\n",
    "\n",
    "all_indicators = {}\n",
    "for params in data_params:\n",
    "    weight_field = params.pop('weight_field')\n",
    "    entity_type = params.pop('entity_type')\n",
    "    dataset = params.pop('dataset')\n",
    "    kwargs = dict(topic_model_path=BASE_PATH.format(dataset) + \"/{}\",\n",
    "                  **BASE_KWARGS, **params)\n",
    "    print(\"Doing\", dataset)\n",
    "    all_indicators[dataset] = generate_indicators(**kwargs)\n",
    "    if weight_field is not None:\n",
    "        all_indicators[f'{dataset}-funding'] = generate_indicators(weight_field=weight_field, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96399"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nuts_lookup = get_nuts_info_lookup()\n",
    "entity_type = {'cordis': 'projects',\n",
    "               'nih': 'projects',\n",
    "               'arxiv': 'articles',\n",
    "               'cordis-funding': 'funding [€]',\n",
    "               'nih-funding': 'funding [$]'}\n",
    "\n",
    "output = defaultdict(list)\n",
    "i = 0\n",
    "for ds_name, ctry_code, indc_name, topic_name, indc_value in flatten(all_indicators):\n",
    "    if pd.isnull(indc_value) or indc_value == 0:\n",
    "        continue    \n",
    "    i += 1\n",
    "    #if ctry_code in ('AW',):\n",
    "    #    continue\n",
    "    indc_desc = verbose_indicator_names[indc_name].format(entity_type[ds_name])\n",
    "    \n",
    "    if ctry_code.startswith('iso_'):\n",
    "        ctry_code = ctry_code[4:]\n",
    "        nuts_level = 0\n",
    "        nuts_name = country_iso_code_to_name(ctry_code, iso2=True)\n",
    "        filename = 'by-country.csv'        \n",
    "    else:\n",
    "        nuts_level = len(ctry_code) - 1\n",
    "        nuts_name = nuts_lookup[ctry_code]['nuts_name']\n",
    "        filename = f'nuts-{nuts_level}.csv'\n",
    "    pseudofile = output[f'{ds_name}/{topic_name}/{filename}']\n",
    "    pseudofile.append({'indicator_name': indc_name, 'indicator_value': indc_value, 'indicator_description': indc_desc,\n",
    "                       'nuts_level': nuts_level, 'nuts_code': ctry_code, 'nuts_name': nuts_name})\n",
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from pathlib import Path\n",
    "\n",
    "s3 = boto3.resource('s3') \n",
    "all_paths = []\n",
    "for path, data in output.items():\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df.sort_values(by=[\"indicator_name\", \"nuts_level\", \"nuts_code\"]).reset_index(drop=True).dropna(axis=0)\n",
    "    if len(df) == 0:\n",
    "        continue\n",
    "    Path.mkdir(Path(path).parent, parents=True, exist_ok=True)\n",
    "    df.to_csv(path, index=False)\n",
    "    s3.Bucket('eurito-csv-indicators').upload_file(path, path)\n",
    "    all_paths.append(f'https://eurito-csv-indicators.s3-eu-west-1.amazonaws.com/{path}')\n",
    "    del df\n",
    "\n",
    "pd.DataFrame(all_paths).to_csv(\"indicator-links.csv\", index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(877, 96399)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(output), 96399"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = s3.Bucket('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for obj in b.objects.all():\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import csv\n",
    "# with open('indicator-links.csv','w') as f:\n",
    "#     wr = csv.writer(f, dialect='excel')\n",
    "#     for path in all_paths:\n",
    "#         wr.writerow([path.replace('.s3.', '.s3-eu-west-1.')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('https:/eurito-csv-indicators.s3.amazonaws.com/nih-funding%2Funiversity+center+program+investigators+science%2Fnuts-4.csv')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "https://eurito-csv-indicators.s3-eu-west-1.amazonaws.com/nih-funding/aging+age+older+older_adults+age_related/by-country.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
